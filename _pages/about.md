---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I am currently a last year Ph.D. candidate at Tsinghua University, supervised by Prof. [Xiu Li](https://thusigsiclab.github.io/thu.github.io/).
I was fortunate to be a visiting student at MMLAB@NTU, where I worked with Prof. [Ziwei Liu](https://liuziwei7.github.io/index.html) on the intersection of MLLM and human motion.
Earlier in my Ph.D. journey, I had the opportunity to explore digital human modeling under the kind guidance of Prof. [Yebin Liu](http://www.liuyebin.com/).
My research interests lie in <strong>human motion and interaction synthesis, motion modeling, generative models, and embodied AI</strong>.


_______________________________________________________________________________________________________
<h3>
  <a name="news"></a> ðŸ†•News
</h3>
<div class="mini">
  <li> <strong>[Oct 2025]</strong> One first-author paper about long dance generation accepted by TPAMI!</li>
  <li> <strong>[Jun 2025]</strong> Recognized as <strong>outstanding</strong> reviewer at CVPR 2025!</li>
  <li> <strong>[May 2025]</strong> Our team was awarded a <strong>Gold Medal</strong> at the International Exhibition of Inventions Geneva!</li>  
  <li> <strong>[Jul 2025]</strong> Three papers about motion/video generation ACM MM 2025!</li>
  <li> <strong>[Jun 2025]</strong> Two papers about huamn motion capture and generation by ICCV 2025!</li>
  <li> <strong>[Feb 2025]</strong> One paper about Text-to-Motion Generation with GPT-4Vision Reward is accepted by CVPR 2025!</li>
  <li> <strong>[Dec 2024]</strong> One paper about group dance generation accepted by AAAI!</li>
  <li> <strong>[Sep 2024]</strong> One paper about speech driven motion generation accepted by NeurIPS!</li>
  <li> <strong>[Feb 2024]</strong> One first-author paper about long dance generation accepted by CVPR 2024!</li>
  <li> <strong>[Dec 2023]</strong> One first-author paper about controllable dance generation accepted by ICASSP 2024!</li>
  <li> <strong>[Dec 2023]</strong> Two papers about 3D object grounding and gestures generation are accepted by AAAI 2024!</li>
  <li> <strong>[Jul 2023]</strong> One first-author paper about music driven dance generation accepted by ICCV 2023!</li>
  </ul>
</div>

<style>
table, th, td {
  border: none;
  border-collapse: collapse;
}
</style>


_______________________________________________________________________________________________________

[Please visit [my google scholar profile](https://scholar.google.com/citations?hl=en&user=h1PooycAAAAJ) for the full publication list.]

<h3>
  <a name="Publications"></a> âœ¨Selected Publications
</h3>

<font face="helvetica, ariel, &#39;sans serif&#39;">
        <table cellspacing="0" cellpadding="0" class="noBorder">
           <tbody>
                <tr>
                    <td width="40%">
                        <img width="320" src="../images/Lodgepp_show.gif" border="0">
                            </td>
                    <td>
                            <b>Lodge++: High-quality and Long Dance Generation with Vivid Choreography Patterns</b>
                    <br>
                    <strong>Ronghui Li</strong>, Hongwen Zhang, Yachao Zhang, Yuxiang Zhang,  Youliang Zhang,  Jie Guo, Yan Zhang, Xiu Li, Yebin Liu.
                    <br>
                    <em>IEEE Transactions on Pattern Analysis and Machine Intelligence. (Accepted)</em>
                    <br>
                    [<a href="https://li-ronghui.github.io/lodgepp">Project</a>][<a href="https://ieeexplore.ieee.org/document/11193731">Paper</a>][<a href="https://github.com/li-ronghui/LODGE">Code</a>]
                    </td>
                </tr>
                <tr>
                    <td width="40%">
                        <img width="320" src="../images/Lodge_show.gif" border="0">
                            </td>
                    <td>
                            <b>Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation Guided by the Characteristic Dance Primitives</b>
                    <br>
                    <strong>Ronghui Li</strong>, Yuxiang Zhang, Yachao Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, Xiu Li.
                    <br>
                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)</em>
                    <br>
                    [<a href="https://li-ronghui.github.io/lodge">Project</a>][<a href="http://arxiv.org/abs/2403.10518">Paper</a>][<a href="https://github.com/li-ronghui/LODGE">Code</a>]
                    </td>
                </tr>
                <tr>
                    <td width="40%">
                        <img width="320" src="../images/motionupg.png" border="0">
                            </td>
                    <td>
                            <b>A Motion is Worth a Hybrid Sentence: Taming Language Model for Unified Motion Generation by Fine-grained Planning</b>
                    <br>
                    <strong>Ronghui Li</strong>, Lingxiao Han, Shi Shu, Yueyao Liu, Yukang Lin, Yue Ma, Jie Guo, Ziwei Liu, Xiu Li.
                    <br>
                    <em>ACM International Conference on Multimedia. (ACM MM 2025)</em>
                    <br>
                    [<a href="https://li-ronghui.github.io/motionupg.github.io/">Project page</a>]
                    </td>
                </tr>
                <tr>
                    <td width="40%">
                        <img width="320" src="../images/FineDance.gif" border="0">
                            </td>
                    <td>
                            <b>FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation</b>
                    <br>
                    <strong>Ronghui Li</strong>, Junfan Zhao,  Yachao Zhang, Mingyang Su, Zeping Ren, Han Zhang, Yansong Tang, Xiu Li.
                    <br>
                    <em>IEEE/CVF Conference on International Conference on Computer Vision (ICCV 2023)</em>
                    <br>
                    [<a href="https://li-ronghui.github.io/finedance">Project</a>][<a href="https://arxiv.org/abs/2212.03741">Paper</a>][<a href="https://github.com/li-ronghui/FineDance">Code</a>]
                    </td>
                </tr>
                <tr>
                    <td width="40%">
                        <img width="320" src="../images/groupdancing.gif" border="0">
                            </td>
                    <td>
                            <b>Harmonious Group Choreography with Trajectory-Controllable Diffusion</b>
                    <br>
                    Yuqin Dai, Wanlu Zhu, <strong>Ronghui Li</strong>, Zeping Ren, Xiangzheng Zhou, Xiu Li, Jun Li1, Jian Yang.
                    <br>
                    <em>Association for the Advance of Artificial Intelligence (AAAI 2025)</em>
                    <br>
                    [<a href="https://wanluzhu.github.io/TCDiffusion/">Project</a>][<a href="https://arxiv.org/abs/2403.06189">Paper</a>][<a href="https://github.com/Da1yuqin/TCDiff">Code Comming</a>]
                    </td>
                </tr>
                <tr>
                    <td width="40%">
                        <img width="320" src="../images/MambaTalk.jpg" border="0">
                            </td>
                    <td>
                            <b>MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models</b>
                    <br>
                    Zunnan Xu, Yukang Lin, Haonan Han, Sicheng Yang, <strong>Ronghui Li</strong>, Yachao Zhang, Xiu Li.
                    <br>
                    <em>Annual Conference on Neural Information Processing Systems (NeurIPS 2024)</em>
                    <br>
                    [<a href="https://kkakkkka.github.io/MambaTalk/">Project</a>][<a href="https://arxiv.org/abs/2403.09471">Paper</a>]
                    </td>
                </tr>
                <tr>
                    <td width="40%">
                        <img width="320" src="../images/DanceControl.jpg" border="0">
                            </td>
                    <td>
                            <b>Exploring Multi-Modal Control in Music-Driven Dance Generation</b>
                    <br>
                    <strong>Ronghui Li</strong>, Yuqin Dai, Yachao Zhang, Jun Li, Jian Yang, Jie Guo, Xiu Li.
                    <br>
                    <em>IEEE International Conference on Acoustics, Speech and Signal Processing
 (ICASSP 2024)</em>
                    <br>
                    [<a href="https://arxiv.org/abs/2401.01382">Paper</a>]
                    </td>
                </tr>
                <tr>
                    <td width="40%">
                        <img width="320" src="../images/Text2Avatar.jpg" border="0">
                            </td>
                    <td>
                          <b>Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven Body Controllable Attribute</b>
                    <br>
                    Chaoqun Gong, Yuqin Dai, <strong>Ronghui Li</strong>, Achun Bao, Jun Li, Jian Yang, Yachao Zhang, Xiu Li.
                    <br>
                    <em>IEEE International Conference on Acoustics, Speech and Signal Processing
 (ICASSP 2024)</em>
                    <br>
                    [<a href="https://browse.arxiv.org/abs/2401.00711">Paper</a>]
                    </td>
                </tr>
                <tr>
                    <td width="40%">
                        <img width="320" src="../images/xmatch.jpg" border="0">
                            </td>
                    <td>
                            <b>Cross-Modal Match for Language Conditioned 3D Object Grounding</b>
                    <br>
                    Yachao Zhang, Runze Hu, <strong>Ronghui Li</strong>, Yanyun Qu, Yuan Xie, Xiu Li.
                    <br>
                    <em>Association for the Advance of Artificial Intelligence (AAAI 2024)</em>
                    <br>
                    [<a href="https://github.com/li-ronghui">Paper</a>]
                    </td>
                </tr>
               <tr>
                    <td width="40%">
                        <img width="320" src="../images/gesture.jpg" border="0">
                            </td>
                    <td>
                            <b>Chain of Generation: Multi-Modal Gesture Synthesis via Cascaded Conditional Control</b>
                    <br>
                     Zunnan Xuï¼ŒYachao Zhangï¼ŒSicheng Yangï¼Œ<strong>Ronghui Li</strong>ï¼ŒXiu Li.
                    <br>
                    <em>Association for the Advance of Artificial Intelligence (AAAI 2024)</em>
                    <br>
                   [<a href="https://arxiv.org/abs/2312.15900">Paper</a>]
                    </td>
               </tr>
                    </tbody>
           </table>
</font>


<!-- [Please visit [my google scholar profile](https://scholar.google.com/citations?hl=en&user=h1PooycAAAAJ) for the full publication list.] -->

_______________________________________________________________________________________________________

<!-- <h3>
  <a name="services"></a> ðŸš©Academic Services
</h3>
<div class="mini">
  <ul>
  <li> <strong>Emergency Conference Reviewer</strong>: CVPR </li>
  <li> <strong>Journal Reviewer</strong>: International Journal of Human-Computer Interaction (IJHCI)</li>
  </ul>
</div> -->
